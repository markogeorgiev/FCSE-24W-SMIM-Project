{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Graph Type and Purpose\n",
    "\n",
    "You are constructing a **heterogeneous directed multigraph** using `NetworkX`’s `MultiDiGraph()` to model complex cyber network interactions. This design is particularly effective for advanced cybersecurity applications such as:\n",
    "\n",
    "- **Graph-based threat detection**\n",
    "- **Anomaly identification in multi-modal behaviors**\n",
    "- **Learning embeddings for heterogeneous entities**\n",
    "\n",
    "### Key Characteristics\n",
    "\n",
    "- **Heterogeneous nodes**  \n",
    "  Represents diverse entities: IP addresses, domain names, HTTP URIs, SSL certificate subjects/issuers, protocol violation types, etc.\n",
    "\n",
    "- **Multi-view relationships**  \n",
    "  Multiple directed edge types between the same pair of nodes allow different interaction views (e.g., flows, DNS queries, HTTP requests).\n",
    "\n",
    "- **Directed edges**  \n",
    "  Encode **temporal or causal flow** (e.g., `src_ip ➝ dst_ip`, `IP ➝ domain`), reflecting who initiated what.\n",
    "\n",
    "# Node Types (Entities)\n",
    "\n",
    "Each node represents a real-world entity, extracted from one or more dataset columns:\n",
    "\n",
    "| Node Type         | Source Column(s)    | Description                                                                 |\n",
    "|-------------------|---------------------|-----------------------------------------------------------------------------|\n",
    "| **IP Address**     | `src_ip`, `dst_ip`  | Devices or interfaces on the network (e.g., `192.168.1.37`).                |\n",
    "| **Domain Name**    | `dns_query`         | Fully qualified domain names queried by IPs (e.g., `www.example.com`).      |\n",
    "| **HTTP URI**       | `http_uri`          | HTTP resource paths (e.g., `/login`, `/index.html`).                        |\n",
    "| **SSL Subject**    | `ssl_subject`       | Distinguished Name of the certificate subject (e.g., `/C=US/O=Let's Encrypt`). |\n",
    "| **SSL Issuer**     | `ssl_issuer`        | Distinguished Name of the certificate issuer (e.g., `/C=US/O=Google Trust Services`). |\n",
    "| **Protocol Violation** | `weird_name`     | Descriptive label of detected anomalies (e.g., `bad_TCP_checksum`).         |\n",
    "\n",
    "---\n",
    "\n",
    "# Edge Types (Views)\n",
    "\n",
    "Each directed edge represents an interaction or behavioral relationship, often enriched with protocol metadata:\n",
    "\n",
    "## 1. `flow` — (IP ➝ IP)\n",
    "\n",
    "Represents a network flow between two IP addresses.\n",
    "\n",
    "- **Source:** `src_ip`  \n",
    "- **Target:** `dst_ip`  \n",
    "- **Attributes:**\n",
    "  - `proto`, `service`, `duration`, `conn_state`\n",
    "  - `src_bytes`, `dst_bytes`\n",
    "  - `label`, `attack_type`\n",
    "\n",
    "**Usefulness:**  \n",
    "Defines the **structural backbone** of the graph, enabling analysis of traffic patterns and attack topologies.\n",
    "\n",
    "## 2. `dns_query` — (IP ➝ Domain Name)\n",
    "\n",
    "Represents a DNS lookup initiated by a host.\n",
    "\n",
    "- **Source:** `src_ip`  \n",
    "- **Target:** `dns_query`  \n",
    "- **Attributes:**\n",
    "  - `qclass`, `qtype`, `rcode`\n",
    "  - `dns_AA`, `dns_RD`, `dns_RA`, `dns_rejected`\n",
    "\n",
    "**Usefulness:**  \n",
    "Reveals **host intent** and can indicate access to suspicious or malicious domains.\n",
    "\n",
    "## 3. `http_request` — (IP ➝ HTTP URI)\n",
    "\n",
    "Captures web resource requests made by a host.\n",
    "\n",
    "- **Source:** `src_ip`  \n",
    "- **Target:** `http_uri`  \n",
    "- **Attributes:**\n",
    "  - `method`, `version`, `status_code`\n",
    "  - `trans_depth`, `req_body_len`, `resp_body_len`\n",
    "  - `user_agent`, `orig_mime`, `resp_mime`\n",
    "\n",
    "**Usefulness:**  \n",
    "Reflects **web behavior**; useful for detecting scanning, reconnaissance, and probing activity.\n",
    "\n",
    "## 4. `protocol_violation` — (IP ➝ Violation Label)\n",
    "\n",
    "Links an IP to a protocol anomaly observed during communication.\n",
    "\n",
    "- **Source:** `src_ip`  \n",
    "- **Target:** `weird_name`  \n",
    "- **Attributes:**\n",
    "  - `weird_addl`, `weird_notice`\n",
    "\n",
    "**Usefulness:**  \n",
    "Highlights **anomalous or misconfigured hosts**. Many such events are early indicators of compromise or malicious activity.\n",
    "\n",
    "# Semantic Graph Properties\n",
    "\n",
    "- **IP nodes are central:**  \n",
    "  Most interaction types originate from or are directed to IP addresses, making them critical in graph topology.\n",
    "\n",
    "- **Multi-modal behavioral modeling:**  \n",
    "  Combines HTTP, DNS, SSL, and flow-level information into one unified representation.\n",
    "\n",
    "- **Multi-view learning ready:**  \n",
    "  The graph supports training models on **protocol-specific subgraphs or jointly across views**.\n",
    "\n",
    "- **Temporal/causal interpretation:**  \n",
    "  Directed edges preserve **who initiated the interaction**, enabling traceability and behavioral profiling."
   ],
   "id": "54dcff684f4eaa0f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Grid Search "
   ],
   "id": "da6d540e1af24f20"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T14:28:20.721875Z",
     "start_time": "2025-05-31T14:28:11.679923Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\"))\n",
    "utils_path = os.path.join(parent_dir, \"project_utils\")\n",
    "sys.path.append(utils_path)\n",
    "\n",
    "from project_utils import graph_creator"
   ],
   "id": "5a0b817c7bed6896",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T14:28:35.437027Z",
     "start_time": "2025-05-31T14:28:20.735948Z"
    }
   },
   "cell_type": "code",
   "source": "G, df = graph_creator.create_graph_from_file('../datasets/balanced_train_test_network.csv')",
   "id": "16190ca1c5f2dfcf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph built with 1420 nodes and 2166 edges.\n",
      "Edge types (views) include: {'ssl_subject', 'ssl_issuer', 'dns_query', 'flow', 'http_request', 'protocol_violation'}\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T14:35:57.524207Z",
     "start_time": "2025-05-31T14:35:55.054960Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv(\"../datasets/balanced_train_test_network.csv\")\n",
    "\n",
    "# Filter out rows with known attack types (exclude normal packets)\n",
    "df_attacks = df[df['type'] != '-']\n",
    "\n",
    "# Replace \"-\" and 0 (where appropriate) with NaN for missing-value analysis\n",
    "df_cleaned = df_attacks.replace(\"-\", pd.NA)\n",
    "df_cleaned = df_cleaned.replace(0, pd.NA)\n",
    "\n",
    "# [Q1] How many unique attack types are present?\n",
    "print(\"[Q1] How many unique attack types are present?\")\n",
    "print(\"Answer:\")\n",
    "print(df_cleaned['type'].value_counts())\n",
    "\n",
    "# [Q2] How many attack samples are there total?\n",
    "print(\"\\n[Q2] How many attack samples are there?\")\n",
    "print(\"Answer:\")\n",
    "print(len(df_cleaned))\n",
    "\n",
    "# [Q3] Are any attack types too rare?\n",
    "print(\"\\n[Q3] Are any attack types too rare?\")\n",
    "print(\"Answer:\")\n",
    "print(df_cleaned['type'].value_counts()[df_cleaned['type'].value_counts() < 10])\n",
    "\n",
    "# [Q4] Which categorical feature value distributions differ by attack type? Example: proto\n",
    "print(\"\\n[Q4] Example feature 'proto' value distributions by attack type:\")\n",
    "print(\"Answer:\")\n",
    "print(df_cleaned.groupby('type')['proto'].value_counts(normalize=True).round(2).head(10))\n",
    "\n",
    "# [Q5] Any numerical columns with possible signal?\n",
    "print(\"\\n[Q5] Numerical columns summary (mean/std) to detect signal:\")\n",
    "print(\"Answer:\")\n",
    "numerical_cols = df.select_dtypes(include=['number']).columns\n",
    "print(df_cleaned[numerical_cols].describe().T[['mean', 'std']])\n",
    "\n",
    "# [Q6] Columns with many missing values (either '-' or 0 treated as missing)\n",
    "print(\"\\n[Q6] Columns with most missing values (original '-' or 0 treated as missing):\")\n",
    "print(\"Answer:\")\n",
    "missing_counts = df_attacks.replace(\"-\", pd.NA).replace(0, pd.NA).isna().sum()\n",
    "print(missing_counts.sort_values(ascending=False).head(15))"
   ],
   "id": "c5e5333a7f594142",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Q1] How many unique attack types are present?\n",
      "Answer:\n",
      "type\n",
      "normal        50000\n",
      "injection      6348\n",
      "ransomware     6308\n",
      "ddos           6223\n",
      "scanning       6214\n",
      "xss            6201\n",
      "password       6162\n",
      "dos            6155\n",
      "backdoor       6071\n",
      "mitm            318\n",
      "Name: count, dtype: int64\n",
      "\n",
      "[Q2] How many attack samples are there?\n",
      "Answer:\n",
      "100000\n",
      "\n",
      "[Q3] Are any attack types too rare?\n",
      "Answer:\n",
      "Series([], Name: count, dtype: int64)\n",
      "\n",
      "[Q4] Example feature 'proto' value distributions by attack type:\n",
      "Answer:\n",
      "type       proto\n",
      "backdoor   tcp      1.00\n",
      "           udp      0.00\n",
      "ddos       tcp      0.73\n",
      "           udp      0.27\n",
      "dos        tcp      0.92\n",
      "           udp      0.08\n",
      "injection  tcp      0.97\n",
      "           udp      0.03\n",
      "mitm       udp      0.55\n",
      "           tcp      0.40\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "[Q5] Numerical columns summary (mean/std) to detect signal:\n",
      "Answer:\n",
      "                 mean           std\n",
      "src_port  35967.68125  20759.624415\n",
      "\n",
      "[Q6] Columns with most missing values (original '-' or 0 treated as missing):\n",
      "Answer:\n",
      "ssl_issuer                99991\n",
      "ssl_subject               99991\n",
      "http_orig_mime_types      99984\n",
      "http_request_body_len     99984\n",
      "weird_addl                99843\n",
      "http_resp_mime_types      99827\n",
      "http_response_body_len    99754\n",
      "http_uri                  99750\n",
      "http_user_agent           99750\n",
      "http_method               99750\n",
      "http_status_code          99736\n",
      "http_version              99736\n",
      "http_trans_depth          99734\n",
      "weird_name                99645\n",
      "weird_notice              99645\n",
      "dtype: int64\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T13:20:59.475143Z",
     "start_time": "2025-05-31T13:20:59.431358Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Convert flow_G (DiGraph) to PyG format\n",
    "ip_nodes = [n for n in G.nodes if isinstance(n, str) and '.' in n]\n",
    "\n",
    "node_to_idx = {node: i for i, node in enumerate(ip_nodes)}\n",
    "edge_index = []\n",
    "\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "for node in ip_nodes:\n",
    "    out_deg = len([1 for _, _, k in G.out_edges(node, keys=True) if k == \"flow\"])\n",
    "    in_deg = len([1 for _, _, k in G.in_edges(node, keys=True) if k == \"flow\"])\n",
    "\n",
    "    features.append([in_deg, out_deg])\n",
    "\n",
    "    label = \"normal\"\n",
    "    for _, _, k, d in G.out_edges(node, keys=True, data=True):\n",
    "        if k == \"flow\" and d.get(\"label\"):\n",
    "            label = \"Attack\" if str(d[\"label\"]).lower() != \"normal\" else \"Normal\"\n",
    "            break\n",
    "    labels.append(label)\n",
    "\n",
    "# Encode features and labels\n",
    "X = StandardScaler().fit_transform(features)\n",
    "y = LabelEncoder().fit_transform(labels)\n",
    "X = torch.tensor(X, dtype=torch.float)\n",
    "y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "# Build edge index\n",
    "for u, v in G.edges():\n",
    "    if u in node_to_idx and v in node_to_idx:\n",
    "        edge_index.append([node_to_idx[u], node_to_idx[v]])\n",
    "\n",
    "edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "\n",
    "# Define PyG Data\n",
    "data = Data(x=X, edge_index=edge_index, y=y)"
   ],
   "id": "860883ce1bb8aac5",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T18:07:32.352244Z",
     "start_time": "2025-05-07T18:07:32.269175Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "# Split train/test\n",
    "torch.manual_seed(42)\n",
    "num_nodes = data.num_nodes\n",
    "perm = torch.randperm(num_nodes)\n",
    "train_idx = perm[:int(0.8 * num_nodes)]\n",
    "test_idx = perm[int(0.8 * num_nodes):]\n",
    "\n",
    "data.train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "data.train_mask[train_idx] = True\n",
    "data.test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "data.test_mask[test_idx] = True\n",
    "\n",
    "\n",
    "# Convert flow_G (DiGraph) to PyG format\n",
    "ip_nodes = [n for n in G.nodes if isinstance(n, str) and '.' in n]\n",
    "\n",
    "node_to_idx = {node: i for i, node in enumerate(ip_nodes)}\n",
    "edge_index = []\n",
    "\n",
    "features = []\n",
    "labels = []\n",
    "\n",
    "for node in ip_nodes:\n",
    "    out_deg = len([1 for _, _, k in G.out_edges(node, keys=True) if k == \"flow\"])\n",
    "    in_deg = len([1 for _, _, k in G.in_edges(node, keys=True) if k == \"flow\"])\n",
    "\n",
    "    features.append([in_deg, out_deg])\n",
    "\n",
    "    label = \"Normal\"\n",
    "    for _, _, k, d in G.out_edges(node, keys=True, data=True):\n",
    "        if k == \"flow\" and d.get(\"label\"):\n",
    "            label = \"Attack\" if str(d[\"label\"]).lower() != \"normal\" else \"Normal\"\n",
    "            break\n",
    "    labels.append(label)\n",
    "\n",
    "# Encode features and labels\n",
    "X = StandardScaler().fit_transform(features)\n",
    "y = LabelEncoder().fit_transform(labels)\n",
    "X = torch.tensor(X, dtype=torch.float)\n",
    "y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "# Build edge index\n",
    "for u, v in G.edges():\n",
    "    if u in node_to_idx and v in node_to_idx:\n",
    "        edge_index.append([node_to_idx[u], node_to_idx[v]])\n",
    "\n",
    "edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "\n",
    "# Define PyG Data\n",
    "data = Data(x=X, edge_index=edge_index, y=y)\n",
    "\n",
    "# Split train/test\n",
    "torch.manual_seed(42)\n",
    "num_nodes = data.num_nodes\n",
    "perm = torch.randperm(num_nodes)\n",
    "train_idx = perm[:int(0.8 * num_nodes)]\n",
    "test_idx = perm[int(0.8 * num_nodes):]\n",
    "\n",
    "data.train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "data.train_mask[train_idx] = True\n",
    "data.test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "data.test_mask[test_idx] = True"
   ],
   "id": "76e4e0112ee59f0c",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T18:08:51.221517Z",
     "start_time": "2025-05-07T18:08:41.222279Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv\n",
    "\n",
    "class GraphSAGE(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "        self.conv1 = SAGEConv(in_channels, hidden_channels)\n",
    "        self.conv2 = SAGEConv(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "\n",
    "def train(model, data, train_mask, optimizer, criterion):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    loss = criterion(out[train_mask], data.y[train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "def test(model, data, test_mask):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(data.x, data.edge_index)\n",
    "        preds = logits[test_mask].argmax(dim=1).cpu()\n",
    "        labels = data.y[test_mask].cpu()\n",
    "\n",
    "    return (\n",
    "        accuracy_score(labels, preds),\n",
    "        precision_score(labels, preds, zero_division=0),\n",
    "        recall_score(labels, preds, zero_division=0),\n",
    "        f1_score(labels, preds, zero_division=0),\n",
    "    )\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "\n",
    "\n",
    "def run_holdout(data, test_sizes=None):\n",
    "    if test_sizes is None:\n",
    "        test_sizes = [0.1, 0.3, 0.5]\n",
    "    results = []\n",
    "    X = data.x.cpu().numpy()\n",
    "    y = data.y.cpu().numpy()\n",
    "\n",
    "    for test_size in test_sizes:\n",
    "        train_idx, test_idx = train_test_split(\n",
    "            range(len(y)), test_size=test_size, stratify=y, random_state=42\n",
    "        )\n",
    "        train_mask = torch.zeros(len(y), dtype=torch.bool)\n",
    "        test_mask = torch.zeros(len(y), dtype=torch.bool)\n",
    "        train_mask[train_idx] = True\n",
    "        test_mask[test_idx] = True\n",
    "\n",
    "        model = GraphSAGE(data.num_node_features, 32, int(data.y.max().item()) + 1).to(data.x.device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "        for epoch in range(100):\n",
    "            train(model, data, train_mask, optimizer, criterion)\n",
    "\n",
    "        acc, prec, rec, f1 = test(model, data, test_mask)\n",
    "        label = f\"{int((1 - test_size) * 100)}/{int(test_size * 100)}\"\n",
    "        results.append((label, acc, prec, rec, f1))\n",
    "    return results\n",
    "\n",
    "\n",
    "def run_cv(data, splits=None):\n",
    "    if splits is None:\n",
    "        splits = [5, 10]\n",
    "    results = []\n",
    "    X = data.x.cpu().numpy()\n",
    "    y = data.y.cpu().numpy()\n",
    "\n",
    "    for k in splits:\n",
    "        skf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "        accs, precs, recs, f1s = [], [], [], []\n",
    "\n",
    "        for train_idx, test_idx in skf.split(X, y):\n",
    "            train_mask = torch.zeros(len(y), dtype=torch.bool)\n",
    "            test_mask = torch.zeros(len(y), dtype=torch.bool)\n",
    "            train_mask[train_idx] = True\n",
    "            test_mask[test_idx] = True\n",
    "\n",
    "            model = GraphSAGE(data.num_node_features, 32, int(data.y.max().item()) + 1).to(data.x.device)\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "            criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "            for epoch in range(100):\n",
    "                train(model, data, train_mask, optimizer, criterion)\n",
    "\n",
    "            acc, prec, rec, f1 = test(model, data, test_mask)\n",
    "            accs.append(acc)\n",
    "            precs.append(prec)\n",
    "            recs.append(rec)\n",
    "            f1s.append(f1)\n",
    "\n",
    "        results.append((str(k), sum(accs) / k, sum(precs) / k, sum(recs) / k, sum(f1s) / k))\n",
    "    return results\n",
    "\n",
    "\n",
    "# Run evaluations\n",
    "holdout_results = run_holdout(data)\n",
    "cv_results = run_cv(data)\n",
    "\n",
    "print(\"Split/CV,Accuracy,precision,recal,f1-score\")\n",
    "for r in holdout_results + cv_results:\n",
    "    print(f\"{r[0]},{r[1]:.4f},{r[2]:.4f},{r[3]:.4f},{r[4]:.4f}\")"
   ],
   "id": "6a4ee85fae72f804",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split/CV,Accuracy,precision,recal,f1-score\n",
      "90/10,0.9925,0.9924,1.0000,0.9962\n",
      "70/30,0.9874,0.9874,1.0000,0.9936\n",
      "50/50,0.9909,0.9909,1.0000,0.9954\n",
      "5,0.9894,0.9909,0.9985,0.9947\n",
      "10,0.9894,0.9909,0.9985,0.9947\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# WTF IS THIS",
   "id": "67ab688f51787c70"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## CV and Grid-Search for GCN",
   "id": "a153701df99d133e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T18:13:09.819093Z",
     "start_time": "2025-05-07T18:12:18.576143Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from itertools import product\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "\n",
    "def train(model, data, train_idx, test_idx, epochs=100, lr=0.01):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    data.train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "    data.train_mask[train_idx] = True\n",
    "    data.test_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "    data.test_mask[test_idx] = True\n",
    "\n",
    "    for _ in range(epochs):\n",
    "        model.train()\n",
    "        out = model(data.x, data.edge_index)\n",
    "        loss = F.cross_entropy(out[data.train_mask], data.y[data.train_mask])\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(data.x, data.edge_index)\n",
    "        pred = logits[data.test_mask].argmax(dim=1)\n",
    "        true = data.y[data.test_mask]\n",
    "        report = classification_report(true.cpu(), pred.cpu(), target_names=[\"Normal\", \"Attack\"], output_dict=True,\n",
    "                                       zero_division=0)\n",
    "        return report['accuracy'], report['weighted avg']['f1-score']\n",
    "\n",
    "\n",
    "# Create balanced dataset (as before)\n",
    "normal_idx = (y == 0).nonzero(as_tuple=True)[0]\n",
    "attack_idx = (y == 1).nonzero(as_tuple=True)[0]\n",
    "num_normals = len(normal_idx)\n",
    "undersampled_attack_idx = attack_idx[torch.randperm(len(attack_idx))[:num_normals]]\n",
    "balanced_idx = torch.cat([normal_idx, undersampled_attack_idx])\n",
    "balanced_idx = balanced_idx[torch.randperm(len(balanced_idx))]\n",
    "\n",
    "X_np = data.x[balanced_idx].cpu().numpy()  # just to satisfy StratifiedKFold\n",
    "y_np = y[balanced_idx].cpu().numpy()\n",
    "\n",
    "# Grid Search with Cross-Validation\n",
    "hidden_sizes = [8, 16]\n",
    "learning_rates = [0.02, 0.03, 0.05]\n",
    "epochs_list = [50, 100, 200]\n",
    "grid = list(product(hidden_sizes, learning_rates, epochs_list))\n",
    "\n",
    "best_f1 = -1\n",
    "best_params = None\n",
    "\n",
    "for hidden, lr, epochs in grid:\n",
    "    fold_f1s = []\n",
    "    fold_acc = []\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    for train_idx_np, test_idx_np in skf.split(X_np, y_np):\n",
    "        train_idx = balanced_idx[train_idx_np]\n",
    "        test_idx = balanced_idx[test_idx_np]\n",
    "\n",
    "        torch.manual_seed(42)\n",
    "        model = GCN(2, hidden, 2)\n",
    "        acc, f1 = train(model, data, train_idx, test_idx, epochs=epochs, lr=lr)\n",
    "        fold_f1s.append(f1)\n",
    "        fold_acc.append(acc)\n",
    "\n",
    "    avg_f1 = np.mean(fold_f1s)\n",
    "    avg_acc = np.mean(fold_acc)\n",
    "    print(f\"GCN(hid={hidden}, lr={lr}, epochs={epochs}) → CV acc: {avg_acc:.4f}, F1: {avg_f1:.4f}\")\n",
    "\n",
    "    if avg_f1 > best_f1:\n",
    "        best_f1 = avg_f1\n",
    "        best_params = (hidden, lr, epochs)\n",
    "\n",
    "print(\n",
    "    f\"\\nBest GCN config: hidden={best_params[0]}, lr={best_params[1]}, epochs={best_params[2]} with CV F1={best_f1:.4f}\")"
   ],
   "id": "580ecc85872b18b3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(hid=8, lr=0.02, epochs=50) → CV acc: 0.7393, F1: 0.7216\n",
      "GCN(hid=8, lr=0.02, epochs=100) → CV acc: 0.7893, F1: 0.7810\n",
      "GCN(hid=8, lr=0.02, epochs=200) → CV acc: 0.7393, F1: 0.7276\n",
      "GCN(hid=8, lr=0.03, epochs=50) → CV acc: 0.7893, F1: 0.7810\n",
      "GCN(hid=8, lr=0.03, epochs=100) → CV acc: 0.7893, F1: 0.7810\n",
      "GCN(hid=8, lr=0.03, epochs=200) → CV acc: 0.7143, F1: 0.7022\n",
      "GCN(hid=8, lr=0.05, epochs=50) → CV acc: 0.7893, F1: 0.7810\n",
      "GCN(hid=8, lr=0.05, epochs=100) → CV acc: 0.7929, F1: 0.7794\n",
      "GCN(hid=8, lr=0.05, epochs=200) → CV acc: 0.6893, F1: 0.6743\n",
      "GCN(hid=16, lr=0.02, epochs=50) → CV acc: 0.7143, F1: 0.6949\n",
      "GCN(hid=16, lr=0.02, epochs=100) → CV acc: 0.7893, F1: 0.7810\n",
      "GCN(hid=16, lr=0.02, epochs=200) → CV acc: 0.7357, F1: 0.7184\n",
      "GCN(hid=16, lr=0.03, epochs=50) → CV acc: 0.7643, F1: 0.7556\n",
      "GCN(hid=16, lr=0.03, epochs=100) → CV acc: 0.7393, F1: 0.7276\n",
      "GCN(hid=16, lr=0.03, epochs=200) → CV acc: 0.7393, F1: 0.7276\n",
      "GCN(hid=16, lr=0.05, epochs=50) → CV acc: 0.7643, F1: 0.7470\n",
      "GCN(hid=16, lr=0.05, epochs=100) → CV acc: 0.7393, F1: 0.7276\n",
      "GCN(hid=16, lr=0.05, epochs=200) → CV acc: 0.7393, F1: 0.7327\n",
      "\n",
      "Best GCN config: hidden=8, lr=0.02, epochs=100 with CV F1=0.7810\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Grid Search with CV on GraphSAGE",
   "id": "d82720b16cecb9ff"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T13:21:24.599065Z",
     "start_time": "2025-05-31T13:21:24.381023Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from itertools import product\n",
    "from sklearn.metrics import classification_report\n",
    "from torch_geometric.nn import SAGEConv\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# GraphSAGE Model Definition\n",
    "class GraphSAGE(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(in_channels, hidden_channels)\n",
    "        self.conv2 = SAGEConv(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Compute class weights based on imbalance\n",
    "def get_class_weights(y):\n",
    "    counts = torch.bincount(y)\n",
    "    weights = 1.0 / counts.float()\n",
    "    weights = weights * (len(y) / weights.sum())  # normalize\n",
    "    return weights\n",
    "\n",
    "\n",
    "# Training Function (same logic)\n",
    "def train(model, data, epochs=100, lr=0.01):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    class_weights = get_class_weights(data.y[data.train_mask]).to(data.x.device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        out = model(data.x, data.edge_index)\n",
    "        loss = F.cross_entropy(out[data.train_mask], data.y[data.train_mask], weight=class_weights)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(data.x, data.edge_index)\n",
    "        pred = logits[data.test_mask].argmax(dim=1)\n",
    "        true = data.y[data.test_mask]\n",
    "        report = classification_report(true.cpu(), pred.cpu(), target_names=[\"Normal\", \"Attack\"], output_dict=True,\n",
    "                                       zero_division=0)\n",
    "        return report['accuracy'], report['weighted avg']['f1-score']\n",
    "\n",
    "\n",
    "# Grid Search Parameters\n",
    "hidden_sizes = [8, 16, 32]\n",
    "learning_rates = [0.01, 0.02, 0.03]\n",
    "epochs_list = [100, 200, 150]\n",
    "\n",
    "grid = list(product(hidden_sizes, learning_rates, epochs_list))\n",
    "\n",
    "best_f1 = -1\n",
    "best_params = None\n",
    "\n",
    "# Grid Search Execution for GraphSAGE\n",
    "for hidden, lr, epochs in grid:\n",
    "    torch.manual_seed(42)\n",
    "    model = GraphSAGE(2, hidden, 2)\n",
    "    acc, f1 = train(model, data, epochs=epochs, lr=lr)\n",
    "    print(f\"GraphSAGE(hid={hidden}, lr={lr}, epochs={epochs}) → Acc: {acc:.4f}, F1: {f1:.4f}\")\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_params = (hidden, lr, epochs)\n",
    "\n",
    "print(\n",
    "    f\"\\nBest GraphSAGE config: hidden={best_params[0]}, lr={best_params[1]}, epochs={best_params[2]} with F1={best_f1:.4f}\")"
   ],
   "id": "86c554547c76f42a",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GlobalStorage' object has no attribute 'train_mask'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mAttributeError\u001B[39m                            Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[6]\u001B[39m\u001B[32m, line 67\u001B[39m\n\u001B[32m     65\u001B[39m torch.manual_seed(\u001B[32m42\u001B[39m)\n\u001B[32m     66\u001B[39m model = GraphSAGE(\u001B[32m2\u001B[39m, hidden, \u001B[32m2\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m67\u001B[39m acc, f1 = \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlr\u001B[49m\u001B[43m=\u001B[49m\u001B[43mlr\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     68\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mGraphSAGE(hid=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mhidden\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m, lr=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlr\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m, epochs=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mepochs\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m) → Acc: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00macc\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m, F1: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mf1\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m     69\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m f1 > best_f1:\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[6]\u001B[39m\u001B[32m, line 33\u001B[39m, in \u001B[36mtrain\u001B[39m\u001B[34m(model, data, epochs, lr)\u001B[39m\n\u001B[32m     31\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mtrain\u001B[39m(model, data, epochs=\u001B[32m100\u001B[39m, lr=\u001B[32m0.01\u001B[39m):\n\u001B[32m     32\u001B[39m     optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n\u001B[32m---> \u001B[39m\u001B[32m33\u001B[39m     class_weights = get_class_weights(data.y[\u001B[43mdata\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtrain_mask\u001B[49m]).to(data.x.device)\n\u001B[32m     35\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(epochs):\n\u001B[32m     36\u001B[39m         model.train()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\FCSE-24W-SMIM-Project\\.venv\\Lib\\site-packages\\torch_geometric\\data\\data.py:561\u001B[39m, in \u001B[36mData.__getattr__\u001B[39m\u001B[34m(self, key)\u001B[39m\n\u001B[32m    555\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[33m'\u001B[39m\u001B[33m_store\u001B[39m\u001B[33m'\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.\u001B[34m__dict__\u001B[39m:\n\u001B[32m    556\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[32m    557\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mThe \u001B[39m\u001B[33m'\u001B[39m\u001B[33mdata\u001B[39m\u001B[33m'\u001B[39m\u001B[33m object was created by an older version of PyG. \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    558\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mIf this error occurred while loading an already existing \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    559\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mdataset, remove the \u001B[39m\u001B[33m'\u001B[39m\u001B[33mprocessed/\u001B[39m\u001B[33m'\u001B[39m\u001B[33m directory in the dataset\u001B[39m\u001B[33m'\u001B[39m\u001B[33ms \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    560\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mroot folder and try again.\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m561\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_store\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\FCSE-24W-SMIM-Project\\.venv\\Lib\\site-packages\\torch_geometric\\data\\storage.py:96\u001B[39m, in \u001B[36mBaseStorage.__getattr__\u001B[39m\u001B[34m(self, key)\u001B[39m\n\u001B[32m     94\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m[key]\n\u001B[32m     95\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m96\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\n\u001B[32m     97\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m.\u001B[34m__class__\u001B[39m.\u001B[34m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m'\u001B[39m\u001B[33m object has no attribute \u001B[39m\u001B[33m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkey\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m'\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m     98\u001B[39m     ) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[31mAttributeError\u001B[39m: 'GlobalStorage' object has no attribute 'train_mask'"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9e6add3e8fa23223"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
