{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-31T18:24:14.365731Z",
     "start_time": "2025-05-31T18:24:14.360310Z"
    }
   },
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch_geometric.data import Data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import NNConv\n",
    "import networkx as nx\n",
    "\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\"))\n",
    "utils_path = os.path.join(parent_dir, \"project_utils\")\n",
    "sys.path.append(utils_path)\n",
    "\n",
    "CSV_PATH = '../datasets/data/NF-ToN-IoT.csv'"
   ],
   "outputs": [],
   "execution_count": 88
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T18:24:14.685202Z",
     "start_time": "2025-05-31T18:24:14.681064Z"
    }
   },
   "cell_type": "code",
   "source": "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")",
   "id": "42faf75d5747d9a7",
   "outputs": [],
   "execution_count": 89
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T18:24:17.273244Z",
     "start_time": "2025-05-31T18:24:15.014109Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.read_csv(CSV_PATH)\n",
    "df = df.sample(frac=0.05, random_state=42)  # 30% of rows\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"label\"] = label_encoder.fit_transform(df[\"Label\"])"
   ],
   "id": "e7c7920bc9d17944",
   "outputs": [],
   "execution_count": 90
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T18:24:17.340049Z",
     "start_time": "2025-05-31T18:24:17.278271Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df[\"src_node\"] = df[\"IPV4_SRC_ADDR\"].astype(str) + \":\" + df[\"L4_SRC_PORT\"].astype(str)\n",
    "df[\"dst_node\"] = df[\"IPV4_DST_ADDR\"].astype(str) + \":\" + df[\"L4_DST_PORT\"].astype(str)"
   ],
   "id": "dce37a30e7b60ebb",
   "outputs": [],
   "execution_count": 91
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T18:24:17.621672Z",
     "start_time": "2025-05-31T18:24:17.520828Z"
    }
   },
   "cell_type": "code",
   "source": [
    "unique_src_ips = df[\"IPV4_SRC_ADDR\"].unique()\n",
    "new_ips = np.random.permutation(unique_src_ips)\n",
    "ip_map = dict(zip(unique_src_ips, new_ips))\n",
    "df[\"IPV4_SRC_ADDR\"] = df[\"IPV4_SRC_ADDR\"].map(ip_map)\n",
    "df[\"IPV4_DST_ADDR\"] = df[\"IPV4_DST_ADDR\"].map(lambda x: ip_map.get(x, x))\n",
    "# Recompute node strings after remapping\n",
    "df[\"src_node\"] = df[\"IPV4_SRC_ADDR\"].astype(str) + \":\" + df[\"L4_SRC_PORT\"].astype(str)\n",
    "df[\"dst_node\"] = df[\"IPV4_DST_ADDR\"].astype(str) + \":\" + df[\"L4_DST_PORT\"].astype(str)"
   ],
   "id": "1a486fd27483d662",
   "outputs": [],
   "execution_count": 92
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T18:24:17.867454Z",
     "start_time": "2025-05-31T18:24:17.794042Z"
    }
   },
   "cell_type": "code",
   "source": [
    "all_nodes = pd.Index(df[\"src_node\"].tolist() + df[\"dst_node\"].tolist()).unique()\n",
    "ip_to_idx = {node: idx for idx, node in enumerate(all_nodes)}\n",
    "num_nodes = len(all_nodes)"
   ],
   "id": "b81e90d36708cf77",
   "outputs": [],
   "execution_count": 93
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T18:24:18.183297Z",
     "start_time": "2025-05-31T18:24:18.139407Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pkt_pair = {}\n",
    "byte_pair = {}\n",
    "flow_pair = {}\n",
    "node_total_flows = {node: 0 for node in all_nodes}\n",
    "node_attack_flows = {node: 0 for node in all_nodes}"
   ],
   "id": "e7d33d89f05ac9f9",
   "outputs": [],
   "execution_count": 94
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T18:24:23.592048Z",
     "start_time": "2025-05-31T18:24:18.275004Z"
    }
   },
   "cell_type": "code",
   "source": [
    "edge_list = []\n",
    "edge_attr_list = []\n",
    "for _, row in df.iterrows():\n",
    "    src = row[\"src_node\"]\n",
    "    dst = row[\"dst_node\"]\n",
    "    src_idx = ip_to_idx[src]\n",
    "    dst_idx = ip_to_idx[dst]\n",
    "    in_pkts = row[\"IN_PKTS\"]\n",
    "    out_pkts = row[\"OUT_PKTS\"]\n",
    "    in_bytes = row[\"IN_BYTES\"]\n",
    "    out_bytes = row[\"OUT_BYTES\"]\n",
    "    duration = row[\"FLOW_DURATION_MILLISECONDS\"]\n",
    "    flags = row[\"TCP_FLAGS\"]\n",
    "    l7 = row[\"L7_PROTO\"]\n",
    "    protocol = row[\"PROTOCOL\"]\n",
    "    total_pkts = in_pkts + out_pkts\n",
    "    total_bytes = in_bytes + out_bytes\n",
    "    # Update edge counters\n",
    "    key = (src_idx, dst_idx)\n",
    "    pkt_pair[key] = pkt_pair.get(key, 0) + total_pkts\n",
    "    byte_pair[key] = byte_pair.get(key, 0) + total_bytes\n",
    "    flow_pair[key] = flow_pair.get(key, 0) + 1\n",
    "    # Update node flow counts\n",
    "    node_total_flows[src] += 1\n",
    "    node_total_flows[dst] += 1\n",
    "    if row[\"label\"] != 0:  # 0=Benign\n",
    "        node_attack_flows[src] += 1\n",
    "        node_attack_flows[dst] += 1\n",
    "    # Temporarily store all flows to decide edges later\n",
    "    edge_list.append((src_idx, dst_idx))\n",
    "    edge_attr_list.append([in_bytes, out_bytes, in_pkts, out_pkts, duration, flags, l7, protocol])"
   ],
   "id": "299ed1d3da0a8f59",
   "outputs": [],
   "execution_count": 95
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T18:24:23.873855Z",
     "start_time": "2025-05-31T18:24:23.772066Z"
    }
   },
   "cell_type": "code",
   "source": [
    "filtered_edges = []\n",
    "filtered_attrs = []\n",
    "for i, (src_idx, dst_idx) in enumerate(edge_list):\n",
    "    if pkt_pair.get((src_idx, dst_idx), 0) >= 10:\n",
    "        filtered_edges.append([src_idx, dst_idx])\n",
    "        filtered_attrs.append(edge_attr_list[i])"
   ],
   "id": "e3854e96a3afad73",
   "outputs": [],
   "execution_count": 96
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T18:24:24.124955Z",
     "start_time": "2025-05-31T18:24:24.076028Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if filtered_edges:\n",
    "    edge_index = torch.tensor(filtered_edges, dtype=torch.long).t().contiguous()\n",
    "    edge_attr = torch.tensor(filtered_attrs, dtype=torch.float)\n",
    "else:\n",
    "    raise ValueError(\"No edges meet the packet threshold.\")"
   ],
   "id": "630474d3c785d25",
   "outputs": [],
   "execution_count": 97
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T18:24:25.015655Z",
     "start_time": "2025-05-31T18:24:24.309900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "G = nx.Graph()\n",
    "edges_np = edge_index.t().tolist()\n",
    "G.add_edges_from(edges_np)"
   ],
   "id": "9ae6f0db61b9c91a",
   "outputs": [],
   "execution_count": 98
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T18:24:39.636552Z",
     "start_time": "2025-05-31T18:24:28.513690Z"
    }
   },
   "cell_type": "code",
   "source": [
    "degree_dict = dict(G.degree())\n",
    "clustering_dict = nx.clustering(G)"
   ],
   "id": "4ca4be8ad352316d",
   "outputs": [],
   "execution_count": 99
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T18:24:39.663330Z",
     "start_time": "2025-05-31T18:24:39.642814Z"
    }
   },
   "cell_type": "code",
   "source": [
    "degree_arr = np.array([degree_dict.get(i, 0) for i in range(num_nodes)], dtype=float)\n",
    "total_pkts_arr = np.zeros(num_nodes, dtype=float)\n",
    "total_bytes_arr = np.zeros(num_nodes, dtype=float)\n",
    "flow_count_arr = np.zeros(num_nodes, dtype=float)\n",
    "attack_fraction_arr = np.zeros(num_nodes, dtype=float)\n",
    "clustering_arr = np.array([clustering_dict.get(i, 0.0) for i in range(num_nodes)], dtype=float)"
   ],
   "id": "ff2351cb0ce8f9d8",
   "outputs": [],
   "execution_count": 100
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T18:24:39.989619Z",
     "start_time": "2025-05-31T18:24:39.863556Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for (u, v), val in pkt_pair.items():\n",
    "    if (u, v) in pkt_pair:\n",
    "        total_pkts_arr[u] += val\n",
    "        total_pkts_arr[v] += val\n",
    "for (u, v), val in byte_pair.items():\n",
    "    if (u, v) in byte_pair:\n",
    "        total_bytes_arr[u] += val\n",
    "        total_bytes_arr[v] += val"
   ],
   "id": "1857b5be5f2d804f",
   "outputs": [],
   "execution_count": 101
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T18:24:40.195728Z",
     "start_time": "2025-05-31T18:24:40.155268Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i, node in enumerate(all_nodes):\n",
    "    total = node_total_flows[node]\n",
    "    attacks = node_attack_flows[node]\n",
    "    flow_count_arr[i] = total\n",
    "    if total > 0:\n",
    "        attack_fraction_arr[i] = attacks / total\n",
    "    else:\n",
    "        attack_fraction_arr[i] = 0.0"
   ],
   "id": "f16cc07af02656e9",
   "outputs": [],
   "execution_count": 102
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T18:24:40.358015Z",
     "start_time": "2025-05-31T18:24:40.352587Z"
    }
   },
   "cell_type": "code",
   "source": [
    "node_stats = np.vstack([\n",
    "    degree_arr,\n",
    "    total_pkts_arr,\n",
    "    total_bytes_arr,\n",
    "    flow_count_arr,\n",
    "    attack_fraction_arr,\n",
    "    clustering_arr\n",
    "]).T  "
   ],
   "id": "17cb076332788b0d",
   "outputs": [],
   "execution_count": 103
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T18:24:42.142408Z",
     "start_time": "2025-05-31T18:24:42.131450Z"
    }
   },
   "cell_type": "code",
   "source": [
    "scaler = StandardScaler()\n",
    "x = scaler.fit_transform(node_stats)"
   ],
   "id": "993ad4f45d2d246f",
   "outputs": [],
   "execution_count": 104
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T18:24:42.968013Z",
     "start_time": "2025-05-31T18:24:42.909632Z"
    }
   },
   "cell_type": "code",
   "source": [
    "node_labels = np.zeros(num_nodes, dtype=int)\n",
    "for i, node in enumerate(all_nodes):\n",
    "    total = node_total_flows[node]\n",
    "    attacks = node_attack_flows[node]\n",
    "    if total > 0 and (attacks / total) >= 0.30:\n",
    "        node_labels[i] = 1\n",
    "    else:\n",
    "        node_labels[i] = 0"
   ],
   "id": "1ad95f697274c28a",
   "outputs": [],
   "execution_count": 105
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T18:24:44.458599Z",
     "start_time": "2025-05-31T18:24:44.435105Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "edge_attr_np = edge_attr.cpu().numpy()\n",
    "edge_attr_np = scaler.fit_transform(edge_attr_np)\n",
    "edge_attr = torch.tensor(edge_attr_np, dtype=torch.float)\n",
    "\n",
    "edge_attr = edge_attr.to(device)"
   ],
   "id": "f6884b09693efb22",
   "outputs": [],
   "execution_count": 106
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T18:24:45.234348Z",
     "start_time": "2025-05-31T18:24:45.188219Z"
    }
   },
   "cell_type": "code",
   "source": [
    "idx = np.arange(data.num_nodes)\n",
    "train_idx, test_idx = train_test_split(idx, test_size=0.3, stratify=data.y)\n",
    "train_idx, val_idx = train_test_split(train_idx, test_size=0.15/0.7, stratify=data.y[train_idx])"
   ],
   "id": "5b471ca3df8bf73d",
   "outputs": [],
   "execution_count": 107
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T18:24:47.748794Z",
     "start_time": "2025-05-31T18:24:47.737593Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create PyG Data object (ensure tensors are on device)\n",
    "data = Data(\n",
    "    x=torch.tensor(x, dtype=torch.float).to(device),\n",
    "    edge_index=edge_index.to(device),\n",
    "    edge_attr=edge_attr.to(device),\n",
    "    y=torch.tensor(node_labels, dtype=torch.long).to(device)\n",
    ")\n",
    "\n",
    "train_mask = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "val_mask   = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "test_mask  = torch.zeros(data.num_nodes, dtype=torch.bool)\n",
    "train_mask[train_idx] = True\n",
    "val_mask[val_idx]     = True\n",
    "test_mask[test_idx]   = True\n",
    "\n",
    "data.train_mask = train_mask.to(device)\n",
    "data.val_mask   = val_mask.to(device)\n",
    "data.test_mask  = test_mask.to(device)"
   ],
   "id": "682f66b23c3c2bd3",
   "outputs": [],
   "execution_count": 108
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T18:24:49.063335Z",
     "start_time": "2025-05-31T18:24:49.060455Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "85064169e4b1240a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T18:24:49.561912Z",
     "start_time": "2025-05-31T18:24:49.552809Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class EdgeEnhancedGCN(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, edge_dim):\n",
    "        super().__init__()\n",
    "        # MLP for first NNConv: output should be in_channels * hidden_channels\n",
    "        self.edge_mlp1 = nn.Sequential(\n",
    "            nn.Linear(edge_dim, hidden_channels * in_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_channels * in_channels, in_channels * hidden_channels)\n",
    "        )\n",
    "        self.conv1 = NNConv(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=hidden_channels,\n",
    "            nn=self.edge_mlp1,\n",
    "            aggr=\"mean\"\n",
    "        )\n",
    "\n",
    "        # MLP for second NNConv: output should be hidden_channels * out_channels\n",
    "        self.edge_mlp2 = nn.Sequential(\n",
    "            nn.Linear(edge_dim, hidden_channels * hidden_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_channels * hidden_channels, hidden_channels * out_channels)\n",
    "        )\n",
    "        self.conv2 = NNConv(\n",
    "            in_channels=hidden_channels,\n",
    "            out_channels=out_channels,\n",
    "            nn=self.edge_mlp2,\n",
    "            aggr=\"mean\"\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr = data.x, data.edge_index, data.edge_attr\n",
    "        x = F.relu(self.conv1(x, edge_index, edge_attr))\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv2(x, edge_index, edge_attr)\n",
    "        x = torch.clamp(x, -1e6, 1e6)\n",
    "        return x"
   ],
   "id": "530fdf22375a468b",
   "outputs": [],
   "execution_count": 109
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T18:24:50.197791Z",
     "start_time": "2025-05-31T18:24:50.193057Z"
    }
   },
   "cell_type": "code",
   "source": [
    "in_dim = data.x.shape[1]        # 6 node features\n",
    "hidden_dim = 32  # instead of 64\n",
    "out_dim = 2                     # benign vs malicious\n",
    "edge_dim = data.edge_attr.shape[1]  # 8 numeric edge features"
   ],
   "id": "c8114894872b4132",
   "outputs": [],
   "execution_count": 110
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T18:24:50.562352Z",
     "start_time": "2025-05-31T18:24:50.548857Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = EdgeEnhancedGCN(in_dim, hidden_dim, out_dim, edge_dim)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "loss_fn = nn.CrossEntropyLoss(weight=torch.tensor([\n",
    "    1.0,  # weight for benign\n",
    "    (train_mask.sum().item() / max(1, (node_labels[train_mask] == 1).sum()))  # inverse freq for malicious\n",
    "], dtype=torch.float))"
   ],
   "id": "4320b0112670cbf0",
   "outputs": [],
   "execution_count": 111
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T18:25:37.935173Z",
     "start_time": "2025-05-31T18:24:51.061836Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "best_val_f1 = 0.0\n",
    "epochs_without_improve = 0\n",
    "patience = 15\n",
    "\n",
    "for epoch in range(200):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data)\n",
    "    loss = loss_fn(out[data.train_mask], data.y[data.train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(data)\n",
    "        val_preds = logits[data.val_mask].argmax(dim=1).cpu().numpy()\n",
    "        val_true  = data.y[data.val_mask].cpu().numpy()\n",
    "        val_f1 = f1_score(val_true, val_preds, average=\"weighted\")\n",
    "\n",
    "    if val_f1 > best_val_f1:\n",
    "        best_val_f1 = val_f1\n",
    "        torch.save(model.state_dict(), \"best_model.pt\")\n",
    "        epochs_without_improve = 0\n",
    "    else:\n",
    "        epochs_without_improve += 1\n",
    "        if epochs_without_improve >= patience:\n",
    "            break\n",
    "\n",
    "    print(f\"Epoch {epoch}, Loss: {loss.item():.4f}, Val F1: {val_f1:.4f}\")"
   ],
   "id": "bb653713c4c33eb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.7522, Val F1: 0.0624\n",
      "Epoch 1, Loss: 1.0311, Val F1: 0.9239\n",
      "Epoch 2, Loss: 1.2858, Val F1: 0.9240\n",
      "Epoch 3, Loss: 1.6808, Val F1: 0.9238\n",
      "Epoch 4, Loss: 0.8290, Val F1: 0.9241\n",
      "Epoch 5, Loss: 1.7070, Val F1: 0.9241\n",
      "Epoch 6, Loss: 1.0719, Val F1: 0.9264\n",
      "Epoch 7, Loss: 1.1284, Val F1: 0.9290\n",
      "Epoch 8, Loss: 1.2047, Val F1: 0.9290\n",
      "Epoch 9, Loss: 1.1605, Val F1: 0.9296\n",
      "Epoch 10, Loss: 1.1358, Val F1: 0.9299\n",
      "Epoch 11, Loss: 0.8295, Val F1: 0.9305\n",
      "Epoch 12, Loss: 1.1520, Val F1: 0.9305\n",
      "Epoch 13, Loss: 0.8570, Val F1: 0.9288\n",
      "Epoch 14, Loss: 0.9004, Val F1: 0.9289\n",
      "Epoch 15, Loss: 0.7894, Val F1: 0.9295\n",
      "Epoch 16, Loss: 0.6861, Val F1: 0.9306\n",
      "Epoch 17, Loss: 0.7709, Val F1: 0.9308\n",
      "Epoch 18, Loss: 0.6834, Val F1: 0.9308\n",
      "Epoch 19, Loss: 0.6032, Val F1: 0.9313\n",
      "Epoch 20, Loss: 0.6889, Val F1: 0.9482\n",
      "Epoch 21, Loss: 0.9245, Val F1: 0.9470\n",
      "Epoch 22, Loss: 1.3807, Val F1: 0.9468\n",
      "Epoch 23, Loss: 0.5188, Val F1: 0.9468\n",
      "Epoch 24, Loss: 1.2987, Val F1: 0.9468\n",
      "Epoch 25, Loss: 0.8913, Val F1: 0.9468\n",
      "Epoch 26, Loss: 0.6455, Val F1: 0.9481\n",
      "Epoch 27, Loss: 0.6724, Val F1: 0.9477\n",
      "Epoch 28, Loss: 0.4222, Val F1: 0.9468\n",
      "Epoch 29, Loss: 0.6131, Val F1: 0.9468\n",
      "Epoch 30, Loss: 0.5425, Val F1: 0.9469\n",
      "Epoch 31, Loss: 0.4552, Val F1: 0.9469\n",
      "Epoch 32, Loss: 0.7431, Val F1: 0.9472\n",
      "Epoch 33, Loss: 0.8292, Val F1: 0.9472\n",
      "Epoch 34, Loss: 0.3993, Val F1: 0.9458\n"
     ]
    }
   ],
   "execution_count": 112
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-31T18:25:53.059242Z",
     "start_time": "2025-05-31T18:25:52.799769Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.load_state_dict(torch.load(\"best_model.pt\"))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model(data)\n",
    "    test_preds = logits[test_mask].argmax(dim=1).cpu().numpy()\n",
    "    test_true  = data.y[test_mask].cpu().numpy()\n",
    "    test_f1 = f1_score(test_true, test_preds, average=\"weighted\")\n",
    "    correct = (test_preds == test_true).sum()\n",
    "    acc = correct / len(test_true)\n",
    "\n",
    "print(f\"Test Accuracy: {acc:.4f}, Test Weighted F1: {test_f1:.4f}\")"
   ],
   "id": "2d3d406f7e2dfd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9465, Test Weighted F1: 0.9490\n"
     ]
    }
   ],
   "execution_count": 113
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
